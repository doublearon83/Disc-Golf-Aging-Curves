#Upload and clean age data
MPO_a<-read.csv("C:/Users/ahoward1/Documents/Disc golf analytics/Aging curves/Ages.csv",header=T,sep=",")
clust
require(lmerTest)
install.packages(rvest)
for (i in 1:nrow(nc_dist_type)) {
dist_mat <- dist(clustdm[,1:7], method = nc_dist_type[i,2])
hclust_avg <- hclust(dist_mat, method = 'average')
#plot(hclust_avg)
cut_avg <- cutree(hclust_avg, k = nc_dist_type[i,1])
clss_hc[,i] <- as.numeric(cut_avg)
names(clss_hc)[i] <- paste(nc_dist_type[i,1],nc_dist_type[i,2],sep="_")
}
#plotting GMM results
require(ggplot2)
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels))) + geom_point(size=3)
pp <- apply(clustdm[,c(2,3,5,6)], 2, function(x) tapply(x, clustdm$cluster_labels, mean,na.rm=T))
ls()
head(clustdm)
require(MuMIn)
require(cAIC4)
#finding number of classes
opt_gmm = Optimal_Clusters_GMM(clustdm[,1:7], max_clusters = 10, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
ratr1<-event %>%
html_nodes(".player-rating") %>%
html_text()
#import data
data_nc19 <- read.csv("data_nc19.csv",header=T)
data_nc <- read.csv("data_nc.csv",header=T)
data_nc_comb <- rbind(data_nc19[,c(3,19:26)],data_nc[,c(3,23:30)])
clustd<-data_nc_comb
fhz<-tapply(clustd$Fairway.Hitsz,clustd$Name,mean,na.rm=T)
c1z<-tapply(clustd$Circle.1z,clustd$Name,mean,na.rm=T)
c2z<-tapply(clustd$Circle.2z,clustd$Name,mean,na.rm=T)
sz<-tapply(clustd$Scramblez,clustd$Name,mean,na.rm=T)
c1pz<-tapply(clustd$Circle.1.Puttingz,clustd$Name,mean,na.rm=T)
c2pz<-tapply(clustd$Circle.2.Puttingz,clustd$Name,mean,na.rm=T)
obz<-tapply(clustd$OBz,clustd$Name,mean,na.rm=T)
names<-names(fhz)
#create dataset for clustering
clustdm<-data.frame(fhz,c1z,c2z,sz,c1pz,c2pz,obz,names)
clustdm<-clustdm[table(data_nc_comb$Name)>5,] #removing players who played fewer than 5 events
clustdm<-clustdm[complete.cases(clustdm),]
#number of clusters (classes or player types)
nc <- c(2:6)
dist_type <- c("euclidean","maximum","manhattan")
nc_dist_type <- expand.grid(nc,dist_type)
clss_hc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_dist_type)))
for (i in 1:nrow(nc_dist_type)) {
dist_mat <- dist(clustdm[,1:7], method = nc_dist_type[i,2])
hclust_avg <- hclust(dist_mat, method = 'average')
#plot(hclust_avg)
cut_avg <- cutree(hclust_avg, k = nc_dist_type[i,1])
clss_hc[,i] <- as.numeric(cut_avg)
names(clss_hc)[i] <- paste(nc_dist_type[i,1],nc_dist_type[i,2],sep="_")
}
####################
###GMM clustering###
####################
require(ClusterR)
#finding number of classes
opt_gmm = Optimal_Clusters_GMM(clustdm[,1:7], max_clusters = 10, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
#generate predictions for 2-6 classes
nc <- c(2:6)
clss <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=length(nc)))
for (i in 1:length(nc)) {
#generating classes
gmm = GMM(clustdm[,1:7], nc[i], dist_mode = "maha_dist", seed_mode = "random_subset", km_iter = 10,
em_iter = 10, verbose = F)
#predicting class
pr = predict_GMM(clustdm[,1:7], gmm$centroids, gmm$covariance_matrices, gmm$weights)
#creating vector of class predictions
clss[,i] <- pr$cluster_labels
names(clss)[i] <- paste("nc",nc[i],sep="")
}
require(ggplot2)
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels))) + geom_point(size=3)
require(kernlab)
krn <- c("rbfdot","polydot","laplacedot","anovadot","splinedot")
nc_krn <- expand.grid(nc,krn)
clss_sc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_krn)))
for (i in 1:nrow(nc_krn)) {
sc <- specc(as.matrix(clustdm[,1:7]), centers = nc_krn[i,1], kernel = nc_krn[i,2], kpar = "local")
clss_sc[,i] <- as.numeric(sc@.Data)
names(clss_sc)[i] <- paste(nc_krn[i,1],nc_krn[i,2],sep="_")
}
#plotting SC results
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels_SC))) + geom_point(size=3)
#plotting SC results
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels_SC))) + geom_point(size=3)
str(clustdm_cv)
#combine datasets for cross-validation
clustdm_cv <- cbind(clustdm,clss,clss_sc,clss_hc)
str(clustdm_cv)
?specc
#finding number of classes
opt_gmm = Optimal_Clusters_GMM(clustdm[,1:7], max_clusters = 10, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
#generate predictions for 2-6 classes
nc <- c(2:6) #setting number of clusters
clss <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=length(nc))) #dataframe for output from models
for (i in 1:length(nc)) { #setup loop to run 5 times
#generating classes
gmm = GMM(clustdm[,1:7], nc[i], dist_mode = "maha_dist", seed_mode = "random_subset", km_iter = 10,
em_iter = 10, verbose = F)
#predicting class
pr = predict_GMM(clustdm[,1:7], gmm$centroids, gmm$covariance_matrices, gmm$weights)
#creating vector of class predictions
clss[,i] <- pr$cluster_labels
names(clss)[i] <- paste("nc",nc[i],sep="")
}
#plotting GMM results
require(ggplot2)
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels))) + geom_point(size=3)
pp <- apply(clustdm[,c(2,3,5,6)], 2, function(x) tapply(x, clustdm$cluster_labels, mean,na.rm=T))
barplot(t(pp),beside=T, ylab = "Standardized Statistic (C1,C2, C1P, C2P)", xlab = "Player Type")
#spectral clustering
#predictions for 2-6 classes and kernels
require(kernlab)
krn <- c("rbfdot","polydot","laplacedot","anovadot","splinedot")
nc_krn <- expand.grid(nc,krn)
clss_sc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_krn)))
for (i in 1:nrow(nc_krn)) {
sc <- specc(as.matrix(clustdm[,1:7]), centers = nc_krn[i,1], kernel = nc_krn[i,2], kpar = "local")
clss_sc[,i] <- as.numeric(sc@.Data)
names(clss_sc)[i] <- paste(nc_krn[i,1],nc_krn[i,2],sep="_")
}
#plotting SC results
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels_SC))) + geom_point(size=3)
########################
###k-means clustering###
########################
nc <- c(2:6)
#make dataframe
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
[,i] <- kmout$cluster #call correct column of dataframe
#combine datasets for cross-validation
clustdm_cv <- cbind(clustdm,clss,clss_sc,clss_hc)
#import data
data_nc19 <- read.csv("data_nc19.csv",header=T)
data_nc <- read.csv("data_nc.csv",header=T)
data_nc_comb <- rbind(data_nc19[,c(3,19:26)],data_nc[,c(3,23:30)])
clustd<-data_nc_comb
fhz<-tapply(clustd$Fairway.Hitsz,clustd$Name,mean,na.rm=T)
c1z<-tapply(clustd$Circle.1z,clustd$Name,mean,na.rm=T)
c2z<-tapply(clustd$Circle.2z,clustd$Name,mean,na.rm=T)
sz<-tapply(clustd$Scramblez,clustd$Name,mean,na.rm=T)
c1pz<-tapply(clustd$Circle.1.Puttingz,clustd$Name,mean,na.rm=T)
c2pz<-tapply(clustd$Circle.2.Puttingz,clustd$Name,mean,na.rm=T)
obz<-tapply(clustd$OBz,clustd$Name,mean,na.rm=T)
names<-names(fhz)
#create dataset for clustering
clustdm<-data.frame(fhz,c1z,c2z,sz,c1pz,c2pz,obz,names)
clustdm<-clustdm[table(data_nc_comb$Name)>5,] #removing players who played fewer than 5 events
clustdm<-clustdm[complete.cases(clustdm),]
#number of clusters (classes or player types)
nc <- c(2:6)
dist_type <- c("euclidean","maximum","manhattan")
nc_dist_type <- expand.grid(nc,dist_type)
clss_hc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_dist_type)))
for (i in 1:nrow(nc_dist_type)) {
dist_mat <- dist(clustdm[,1:7], method = nc_dist_type[i,2])
hclust_avg <- hclust(dist_mat, method = 'average')
#plot(hclust_avg)
cut_avg <- cutree(hclust_avg, k = nc_dist_type[i,1])
clss_hc[,i] <- as.numeric(cut_avg)
names(clss_hc)[i] <- paste(nc_dist_type[i,1],nc_dist_type[i,2],sep="_")
}
####################
###GMM clustering###
####################
require(ClusterR)
#finding number of classes
opt_gmm = Optimal_Clusters_GMM(clustdm[,1:7], max_clusters = 10, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
#generate predictions for 2-6 classes
nc <- c(2:6) #setting number of clusters
clss <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=length(nc))) #dataframe for output from models
for (i in 1:length(nc)) { #setup loop to run 5 times
#generating classes
gmm = GMM(clustdm[,1:7], nc[i], dist_mode = "maha_dist", seed_mode = "random_subset", km_iter = 10,
em_iter = 10, verbose = F)
#predicting class
pr = predict_GMM(clustdm[,1:7], gmm$centroids, gmm$covariance_matrices, gmm$weights)
#creating vector of class predictions
clss[,i] <- pr$cluster_labels
names(clss)[i] <- paste("nc",nc[i],sep="")
}
#plotting GMM results
require(ggplot2)
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels))) + geom_point(size=3)
krn <- c("rbfdot","polydot","laplacedot","anovadot","splinedot")
nc_krn <- expand.grid(nc,krn)
clss_sc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_krn)))
for (i in 1:nrow(nc_krn)) {
sc <- specc(as.matrix(clustdm[,1:7]), centers = nc_krn[i,1], kernel = nc_krn[i,2], kpar = "local")
clss_sc[,i] <- as.numeric(sc@.Data)
names(clss_sc)[i] <- paste(nc_krn[i,1],nc_krn[i,2],sep="_")
}
#plotting SC results
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels_SC))) + geom_point(size=3)
clustdm
plot(clustd)
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels_SC))) + geom_point(size=3)
#import data
data_nc19 <- read.csv("data_nc19.csv",header=T)
data_nc <- read.csv("data_nc.csv",header=T)
data_nc_comb <- rbind(data_nc19[,c(3,19:26)],data_nc[,c(3,23:30)])
clustd<-data_nc_comb
fhz<-tapply(clustd$Fairway.Hitsz,clustd$Name,mean,na.rm=T)
c1z<-tapply(clustd$Circle.1z,clustd$Name,mean,na.rm=T)
c2z<-tapply(clustd$Circle.2z,clustd$Name,mean,na.rm=T)
sz<-tapply(clustd$Scramblez,clustd$Name,mean,na.rm=T)
c1pz<-tapply(clustd$Circle.1.Puttingz,clustd$Name,mean,na.rm=T)
c2pz<-tapply(clustd$Circle.2.Puttingz,clustd$Name,mean,na.rm=T)
obz<-tapply(clustd$OBz,clustd$Name,mean,na.rm=T)
names<-names(fhz)
#create dataset for clustering
clustdm<-data.frame(fhz,c1z,c2z,sz,c1pz,c2pz,obz,names)
clustdm<-clustdm[table(data_nc_comb$Name)>5,] #removing players who played fewer than 5 events
clustdm<-clustdm[complete.cases(clustdm),]
#number of clusters (classes or player types)
nc <- c(2:6)
dist_type <- c("euclidean","maximum","manhattan")
nc_dist_type <- expand.grid(nc,dist_type)
clss_hc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_dist_type)))
for (i in 1:nrow(nc_dist_type)) {
dist_mat <- dist(clustdm[,1:7], method = nc_dist_type[i,2])
hclust_avg <- hclust(dist_mat, method = 'average')
#plot(hclust_avg)
cut_avg <- cutree(hclust_avg, k = nc_dist_type[i,1])
clss_hc[,i] <- as.numeric(cut_avg)
names(clss_hc)[i] <- paste(nc_dist_type[i,1],nc_dist_type[i,2],sep="_")
}
####################
###GMM clustering###
####################
require(ClusterR)
#finding number of classes
opt_gmm = Optimal_Clusters_GMM(clustdm[,1:7], max_clusters = 10, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
#generate predictions for 2-6 classes
nc <- c(2:6) #setting number of clusters
clss <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=length(nc))) #dataframe for output from models
for (i in 1:length(nc)) { #setup loop to run 5 times
#generating classes
gmm = GMM(clustdm[,1:7], nc[i], dist_mode = "maha_dist", seed_mode = "random_subset", km_iter = 10,
em_iter = 10, verbose = F)
#predicting class
pr = predict_GMM(clustdm[,1:7], gmm$centroids, gmm$covariance_matrices, gmm$weights)
#creating vector of class predictions
clss[,i] <- pr$cluster_labels
names(clss)[i] <- paste("nc",nc[i],sep="")
}
#plotting GMM results
require(ggplot2)
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels))) + geom_point(size=3)
#spectral clustering
#predictions for 2-6 classes and kernels
require(kernlab)
krn <- c("rbfdot","polydot","laplacedot","anovadot","splinedot")
nc_krn <- expand.grid(nc,krn)
clss_sc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_krn)))
for (i in 1:nrow(nc_krn)) {
sc <- specc(as.matrix(clustdm[,1:7]), centers = nc_krn[i,1], kernel = nc_krn[i,2], kpar = "local")
clss_sc[,i] <- as.numeric(sc@.Data)
names(clss_sc)[i] <- paste(nc_krn[i,1],nc_krn[i,2],sep="_")
}
#plotting SC results
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels_SC))) + geom_point(size=3)
?kmeans
?Optimal_Clusters_GMM
?silhouette_of_clusters
fviz_nbclust(clustdm, kmeans, method = "wss")
library(factoextra)
require(factoextra)
?factoextra
install.packages(factoextra)
install.packages("factoextra")
install.packages("factoextra")
library(factoextra)
fviz_nbclust(clustdm, kmeans, method = "wss")
fviz_nbclust(df, kmeans, method = "wss")
fviz_nbclust(clustdm, kmeans, method = "wss")
fviz_nbclust(data_nc19, kmeans, method = "wss")
#making only numeric values
clustdm <- clustdm[,sapply(clustdm, is.numeric)]
clustdm_scaled <- scale(clustdm)
fviz_nbclust(data_nc19, kmeans, method = "wss")
#making only numeric values
clustdm <- clustdm[,sapply(clustdm, is.numeric)]
clustdm_scaled <- scale(clustdm)
fviz_nbclust(data_nc19, kmeans, method = "wss")
nc <- c(2:6)
#make dataframe
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
#combine datasets for cross-validation
clustdm_cv <- cbind(clustdm,clss,clss_sc,clss_hc)
library(CLUSTDM)
install.packages("CLUSTDM")
updateR
updateR()
library(installr)
updateR()
library(installr)
updateR()
library(CLUSTDM)
install.packages("CLUSTDM")
#combine data needed from pre-2020 and post-2020
#>15 and 3 groups for 3 tiers of players
#>25 and 5 groups for 5 types of players
#import data
data_nc19 <- read.csv("data_nc19.csv",header=T)
data_nc <- read.csv("data_nc.csv",header=T)
data_nc_comb <- rbind(data_nc19[,c(3,19:26)],data_nc[,c(3,23:30)])
clustd<-data_nc_comb
fhz<-tapply(clustd$Fairway.Hitsz,clustd$Name,mean,na.rm=T)
c1z<-tapply(clustd$Circle.1z,clustd$Name,mean,na.rm=T)
c2z<-tapply(clustd$Circle.2z,clustd$Name,mean,na.rm=T)
sz<-tapply(clustd$Scramblez,clustd$Name,mean,na.rm=T)
c1pz<-tapply(clustd$Circle.1.Puttingz,clustd$Name,mean,na.rm=T)
c2pz<-tapply(clustd$Circle.2.Puttingz,clustd$Name,mean,na.rm=T)
obz<-tapply(clustd$OBz,clustd$Name,mean,na.rm=T)
names<-names(fhz)
#create dataset for clustering
clustdm<-data.frame(fhz,c1z,c2z,sz,c1pz,c2pz,obz,names)
clustdm<-clustdm[table(data_nc_comb$Name)>5,] #removing players who played fewer than 5 events
clustdm<-clustdm[complete.cases(clustdm),]
#number of clusters (classes or player types)
nc <- c(2:6)
#############################
###hierarchical clustering###
#############################
dist_type <- c("euclidean","maximum","manhattan")
nc_dist_type <- expand.grid(nc,dist_type)
clss_hc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_dist_type)))
for (i in 1:nrow(nc_dist_type)) {
dist_mat <- dist(clustdm[,1:7], method = nc_dist_type[i,2])
hclust_avg <- hclust(dist_mat, method = 'average')
#plot(hclust_avg)
cut_avg <- cutree(hclust_avg, k = nc_dist_type[i,1])
clss_hc[,i] <- as.numeric(cut_avg)
names(clss_hc)[i] <- paste(nc_dist_type[i,1],nc_dist_type[i,2],sep="_")
}
#require(dendextend)
#suppressPackageStartupMessages(library(dendextend))
#avg_dend_obj <- as.dendrogram(hclust_avg)
#avg_col_dend <- color_branches(avg_dend_obj, h = 5)
#plot(avg_col_dend)
####################
###GMM clustering###
####################
require(ClusterR)
#finding number of classes
opt_gmm = Optimal_Clusters_GMM(clustdm[,1:7], max_clusters = 10, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
nc <- c(2:6)
#make dataframe
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
nc <- c(2:6)
clss_kmeans <- matrix(NA, nrow = nrow(clustdm), ncol = length(nc))
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
clss_kmeans <- as.data.frame(clss_kmeans)
#combine datasets for cross-validation
clustdm_cv <- cbind(clustdm,clss,clss_sc,clss_hc)
ggplot2(clss_kmeans)
library(ggplot2)
ggplot2(clss_kmeans)
plot(clss_kmeans)
# Initialize vector to store WSS for each k
wss <- numeric(length(nc))
# Compute WSS for each number of clusters
for (i in seq_along(nc)) {
kmout <- kmeans(clustdm[,1:7], centers = nc[i], nstart = 20)
wss[i] <- kmout$tot.withinss  # Store within-cluster sum of squares
}
# Plot the elbow method
plot(nc, wss, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Total Within-Cluster Sum of Squares",
main = "Elbow Method for Optimal k")
plot(clustdm)
head(clustdm)
