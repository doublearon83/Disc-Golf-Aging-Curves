clss_kmeans <- matrix(NA, nrow = nrow(clustdm), ncol = length(nc_types))
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_types <- expand.grid(nc, km_types)
clss_kmeans <- matrix(NA, nrow = nrow(clustdm), ncol = length(nc_types))
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_types <- expand.grid(nc, km_types)
clss_kmeans <- matrix(NA, nrow = nrow(clustdm), ncol = length(nc_types))
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
head(nc_types)
head(nc_krn)
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_types <- expand.grid(nc, km_types) #uses expand grid
clss_kmeans <- as.data.frame(matrix(NA, nrow = nrow(clustdm), ncol = length(nc_types)))
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
clss_kmeans <- as.data.frame(clss_kmeans)
head(clss_kmeans)
head(clss_sc)
head(nc_krn)
head(nc_types)
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_types <- expand.grid(nc, km_types) #uses expand grid
clss_kmeans <- as.data.frame(matrix(NA, nrow = nrow(clustdm), ncol = nrow(nc_types)))
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
clss_kmeans <- as.data.frame(clss_kmeans)
head(clss_kmeans)
clss_kmeans <- as.data.frame(matrix(NA, nrow = nrow(clustdm), ncol = length(nc_types)))
head(clss_kmeans)
nc <- c(2:6)
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_types <- expand.grid(nc, km_types) #uses expand grid
clss_kmeans <- as.data.frame(matrix(NA, nrow = nrow(clustdm), ncol = length(nc_types)))
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
clss_kmeans <- as.data.frame(clss_kmeans)
head(clss_kmeans)
nc <- c(2:6)
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_types <- expand.grid(nc, km_types) #uses expand grid
clss_kmeans <- as.data.frame(matrix(NA, nrow = nrow(clustdm), ncol = nrow(nc_types)))
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
clss_kmeans <- as.data.frame(clss_kmeans)
#combine datasets for cross-validation
clustdm_cv <- cbind(clustdm,clss,clss_sc,clss_hc,clss_kmeans)
head(clss_kmeans)
clss_kmeans <- as.data.frame(matrix(NA, nrow = nrow(clustdm), ncol = length(nc_types)))
head(clss_kmeans)
nc <- c(2:6)
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_types <- expand.grid(nc, km_types) #uses expand grid
clss_kmeans <- as.data.frame(matrix(NA, nrow = nrow(clustdm), ncol = length(nc_types)))
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
clss_kmeans <- as.data.frame(clss_kmeans)
#combine datasets for cross-validation
clustdm_cv <- cbind(clustdm,clss,clss_sc,clss_hc,clss_kmeans)
head(clss_kmeans)
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
nc <- c(2:6)
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_types <- expand.grid(nc, km_types) #uses expand grid
clss_kmeans <- as.data.frame(matrix(0, nrow = nrow(clustdm), ncol = nrow(nc_types)))
for (i in 1:length(nc)){
kmout<- kmeans(clustdm[,1:7],centers=nc[i],nstart=20)
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
clss_kmeans <- as.data.frame(clss_kmeans)
head(clss_kmeans)
nc <- c(2:6)
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_kmean <- expand.grid(nc, km_types) #uses expand grid
clss_kmeans <- as.data.frame(matrix(0, nrow = nrow(clustdm), ncol = nrow(nc_kmean)))
for (i in 1:nrow(nc_kmean)){
kmout<- kmeans(as.matrix(clustdm[,1:7],centers=nc_kmean[i,1],nstart=20))
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
nc <- c(2:6)
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_kmean <- expand.grid(nc = nc,km_types = km_types) #uses expand grid
clss_kmeans <- as.data.frame(matrix(0, nrow = nrow(clustdm), ncol = nrow(nc_kmean)))
for (i in 1:nrow(nc_kmean)){
kmout<- kmeans(as.matrix(clustdm[,1:7],
centers=nc_kmean$nc[i],
nstart=20,
algorithm = as.character((nc_kmean$km_type[i]))
))
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
nc <- c(2:6)
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_kmean <- expand.grid(nc = nc,km_types = km_types) #uses expand grid
clss_kmeans <- as.data.frame(matrix(0, nrow = nrow(clustdm), ncol = nrow(nc_kmean)))
for (i in 1:nrow(nc_kmean)){
kmout<- kmeans(as.matrix(clustdm[,1:7]),
centers = as.numeric(nc_kmean$nc[i]),
nstart=20,
algorithm = as.character(nc_kmean$km_type[i]))
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
}
clss_kmeans <- as.data.frame(clss_kmeans)
head(clss_kmeans)
head(clss_sc)
head(clss_hc)
nc <- c(2:6)
km_types <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen") #different grouping types from k means
nc_kmean <- expand.grid(nc = nc,km_types = km_types) #uses expand grid
clss_kmeans <- as.data.frame(matrix(0, nrow = nrow(clustdm), ncol = nrow(nc_kmean)))
for (i in 1:nrow(nc_kmean)){
kmout<- kmeans(as.matrix(clustdm[,1:7]),
centers = as.numeric(nc_kmean$nc[i]),
nstart=20,
algorithm = as.character(nc_kmean$km_type[i]))
clss_kmeans[, i] <- kmout$cluster #call correct column of dataframe
names(clss_kmeans)[i] <- paste(nc_kmean[i,1], nc_kmean[i,2], sep = "_")
}
clss_kmeans <- as.data.frame(clss_kmeans)
head(clss_kmeans)
Error in kmeans(as.matrix(clustdm[, 1:7], centers = nc_kmean$nc[i], nstart = 20,  :
#combine data needed from pre-2020 and post-2020
#>15 and 3 groups for 3 tiers of players
#>25 and 5 groups for 5 types of players
#import data
data_nc19 <- read.csv("data_nc19.csv",header=T)
data_nc <- read.csv("data_nc.csv",header=T)
data_nc_comb <- rbind(data_nc19[,c(3,19:26)],data_nc[,c(3,23:30)])
clustd<-data_nc_comb
fhz<-tapply(clustd$Fairway.Hitsz,clustd$Name,mean,na.rm=T)
c1z<-tapply(clustd$Circle.1z,clustd$Name,mean,na.rm=T)
c2z<-tapply(clustd$Circle.2z,clustd$Name,mean,na.rm=T)
sz<-tapply(clustd$Scramblez,clustd$Name,mean,na.rm=T)
c1pz<-tapply(clustd$Circle.1.Puttingz,clustd$Name,mean,na.rm=T)
c2pz<-tapply(clustd$Circle.2.Puttingz,clustd$Name,mean,na.rm=T)
obz<-tapply(clustd$OBz,clustd$Name,mean,na.rm=T)
names<-names(fhz)
#create dataset for clustering
clustdm<-data.frame(fhz,c1z,c2z,sz,c1pz,c2pz,obz,names)
clustdm<-clustdm[table(data_nc_comb$Name)>5,] #removing players who played fewer than 5 events
clustdm<-clustdm[complete.cases(clustdm),]
#number of clusters (classes or player types)
nc <- c(2:6)
#############################
###hierarchical clustering###
#############################
dist_type <- c("euclidean","maximum","manhattan")
nc_dist_type <- expand.grid(nc,dist_type)
clss_hc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_dist_type)))
for (i in 1:nrow(nc_dist_type)) {
dist_mat <- dist(clustdm[,1:7], method = nc_dist_type[i,2])
hclust_avg <- hclust(dist_mat, method = 'average')
#plot(hclust_avg)
cut_avg <- cutree(hclust_avg, k = nc_dist_type[i,1])
clss_hc[,i] <- as.numeric(cut_avg)
names(clss_hc)[i] <- paste(nc_dist_type[i,1],nc_dist_type[i,2],sep="_")
}
#require(dendextend)
#suppressPackageStartupMessages(library(dendextend))
#avg_dend_obj <- as.dendrogram(hclust_avg)
#avg_col_dend <- color_branches(avg_dend_obj, h = 5)
#plot(avg_col_dend)
####################
###GMM clustering###
####################
require(ClusterR)
#finding number of classes
opt_gmm = Optimal_Clusters_GMM(clustdm[,1:7], max_clusters = 10, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
#generate predictions for 2-6 classes
nc <- c(2:6) #setting number of clusters
clss <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=length(nc))) #dataframe for output from models
for (i in 1:length(nc)) { #setup loop to run 5 times
#generating classes
gmm = GMM(clustdm[,1:7], nc[i], dist_mode = "maha_dist", seed_mode = "random_subset", km_iter = 10,
em_iter = 10, verbose = F)
#predicting class
pr = predict_GMM(clustdm[,1:7], gmm$centroids, gmm$covariance_matrices, gmm$weights)
#creating vector of class predictions
clss[,i] <- pr$cluster_labels
names(clss)[i] <- paste("nc",nc[i],sep="")
}
#plotting GMM results
require(ggplot2)
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels))) + geom_point(size=3)
#combine data needed from pre-2020 and post-2020
#>15 and 3 groups for 3 tiers of players
#>25 and 5 groups for 5 types of players
#import data
data_nc19 <- read.csv("data_nc19.csv",header=T)
data_nc <- read.csv("data_nc.csv",header=T)
data_nc_comb <- rbind(data_nc19[,c(3,19:26)],data_nc[,c(3,23:30)])
clustd<-data_nc_comb
fhz<-tapply(clustd$Fairway.Hitsz,clustd$Name,mean,na.rm=T)
c1z<-tapply(clustd$Circle.1z,clustd$Name,mean,na.rm=T)
c2z<-tapply(clustd$Circle.2z,clustd$Name,mean,na.rm=T)
sz<-tapply(clustd$Scramblez,clustd$Name,mean,na.rm=T)
c1pz<-tapply(clustd$Circle.1.Puttingz,clustd$Name,mean,na.rm=T)
c2pz<-tapply(clustd$Circle.2.Puttingz,clustd$Name,mean,na.rm=T)
obz<-tapply(clustd$OBz,clustd$Name,mean,na.rm=T)
names<-names(fhz)
#create dataset for clustering
clustdm<-data.frame(fhz,c1z,c2z,sz,c1pz,c2pz,obz,names)
clustdm<-clustdm[table(data_nc_comb$Name)>5,] #removing players who played fewer than 5 events
clustdm<-clustdm[complete.cases(clustdm),]
#number of clusters (classes or player types)
nc <- c(2:6)
#############################
###hierarchical clustering###
#############################
dist_type <- c("euclidean","maximum","manhattan")
nc_dist_type <- expand.grid(nc,dist_type)
clss_hc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_dist_type)))
for (i in 1:nrow(nc_dist_type)) {
dist_mat <- dist(clustdm[,1:7], method = nc_dist_type[i,2])
hclust_avg <- hclust(dist_mat, method = 'average')
#plot(hclust_avg)
cut_avg <- cutree(hclust_avg, k = nc_dist_type[i,1])
clss_hc[,i] <- as.numeric(cut_avg)
names(clss_hc)[i] <- paste(nc_dist_type[i,1],nc_dist_type[i,2],sep="_")
}
#require(dendextend)
#suppressPackageStartupMessages(library(dendextend))
#avg_dend_obj <- as.dendrogram(hclust_avg)
#avg_col_dend <- color_branches(avg_dend_obj, h = 5)
#plot(avg_col_dend)
####################
###GMM clustering###
####################
require(ClusterR)
#finding number of classes
opt_gmm = Optimal_Clusters_GMM(clustdm[,1:7], max_clusters = 10, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
#generate predictions for 2-6 classes
nc <- c(2:6) #setting number of clusters
clss <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=length(nc))) #dataframe for output from models
for (i in 1:length(nc)) { #setup loop to run 5 times
#generating classes
gmm = GMM(clustdm[,1:7], nc[i], dist_mode = "maha_dist", seed_mode = "random_subset", km_iter = 10,
em_iter = 10, verbose = F)
#predicting class
pr = predict_GMM(clustdm[,1:7], gmm$centroids, gmm$covariance_matrices, gmm$weights)
#creating vector of class predictions
clss[,i] <- pr$cluster_labels
names(clss)[i] <- paste("nc",nc[i],sep="")
}
#plotting GMM results
require(ggplot2)
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels))) + geom_point(size=3)
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels))) + geom_point(size=3)
pp <- apply(clustdm[,c(2,3,5,6)], 2, function(x) tapply(x, clustdm$cluster_labels, mean,na.rm=T))
ggplot(clss_kmeans)
plot(clss_kmeans)
wss <- numeric(length(nc))  # Store WSS values
for (i in seq_along(nc)) {
kmout <- kmeans(as.matrix(clustdm[, 1:7]), centers = nc[i], nstart = 20)
wss[i] <- kmout$tot.withinss  # Store within-cluster sum of squares
}
# Plot WSS
plot(nc, wss, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)", ylab = "Total Within-Cluster SS",
main = "Elbow Method for Optimal k")
head(clss_kmeans)
require(lme4)
require(lmerTest)
require(MuMIn)
require(cAIC4)
# read in age and PDGA data
MPO_ar <- read.csv("MPO_ar.csv",header=T)
#MPO_ar dataset comes from "MPO_Aging.r" script
MPO_ar$years<-scale(MPO_ar$year)
MPO_ar$ages<-scale(MPO_ar$Age)
#combine ratings and classes (data comes from Player_type.r) for cross-validation of ratings
#can I make this betterc(faster) by pasting all at once then just using match to subset rows?
MPO_ar_tune <- data.frame(matrix(0,nrow=nrow(MPO_ar),ncol=ncol(cbind(clss,clss_sc,clss_hc))))
for (i in 1:nrow(MPO_ar))
{
if (any(grepl(MPO_ar$Name[i],clustdm_cv$names))) {MPO_ar_tune[i,]<-as.numeric(clustdm_cv[grep(paste(MPO_ar$Name_f[i],MPO_ar$Name[i],sep = " "),clustdm_cv$names),c(9:ncol(clustdm_cv))])}
else (MPO_ar_tune[i,]<-"NA")
}
# read in age and PDGA data
MPO_ar <- read.csv("MPO_ar.csv",header=T)
#MPO_ar dataset comes from "MPO_Aging.r" script
MPO_ar$years<-scale(MPO_ar$year)
MPO_ar$ages<-scale(MPO_ar$Age)
#combine ratings and classes (data comes from Player_type.r) for cross-validation of ratings
#can I make this betterc(faster) by pasting all at once then just using match to subset rows?
MPO_ar_tune <- data.frame(matrix(0,nrow=nrow(MPO_ar),ncol=ncol(cbind(clss,clss_sc,clss_hc))))
for (i in 1:nrow(MPO_ar))
{
if (any(grepl(MPO_ar$Name[i],clustdm_cv$names))) {MPO_ar_tune[i,]<-as.numeric(clustdm_cv[grep(paste(MPO_ar$Name_f[i],MPO_ar$Name[i],sep = " "),clustdm_cv$names),c(9:ncol(clustdm_cv))])}
else (MPO_ar_tune[i,]<-"NA")
}
#remove NAs and player type 3 (not enough age data for player type 3)
#MPO_ar_pt <- subset(MPO_ar,MPO_ar$ptype!="NA" & MPO_ar$ptype!="3")
# run cross-validation to compare predictions of age curves including
# player type versus excluding player type
#create dataframe for tuning and remove NAs
MPO_ar_tune2 <- data.frame(MPO_ar,MPO_ar_tune)
names(MPO_ar_tune2)[(ncol(MPO_ar_tune2)-44):ncol(MPO_ar_tune2)] <- names(clustdm_cv)[(ncol(clustdm_cv)-44):ncol(clustdm_cv)]
MPO_ar_pt <- subset(MPO_ar_tune2,MPO_ar_tune2$nc2!="NA")
#run loops
mean_SS_t <- data.frame(matrix(0,nrow=ncol(cbind(clss,clss_sc,clss_hc,clss_kmeans)),ncol=2))
for (j in (ncol(MPO_ar_tune2)-44):ncol(MPO_ar_tune2)) {
sumsq_p<-numeric(1000)
sumsq<-numeric(1000)
#training size
tz <- round(length(unique(MPO_ar_pt$PDGA))*.8)
for (i in 1:1000){
train <- sample(unique(MPO_ar_pt$PDGA),tz)
test <- unique(MPO_ar_pt$PDGA)[!unique(MPO_ar_pt$PDGA) %in% train]
MPO_ar_pt_train <- MPO_ar_pt[MPO_ar_pt$PDGA %in% train,]
MPO_ar_pt_test <- MPO_ar_pt[MPO_ar_pt$PDGA %in% test,]
pt_type <- MPO_ar_pt_train[,j]
pt_ages <- MPO_ar_pt_train$ages
pt_ratr <- MPO_ar_pt_train$ratr
MPO_ar_pt_train2 <- data.frame(pt_type,pt_ages,pt_ratr)
outp<-lm(pt_ratr~pt_type*pt_ages+pt_type*I(pt_ages^2)+pt_type*I(pt_ages^3)+pt_type*I(pt_ages^4),data=MPO_ar_pt_train2)
pt_type <- MPO_ar_pt_test[,j]
pt_ages <- MPO_ar_pt_test$ages
pt_ratr <- MPO_ar_pt_test$ratr
MPO_ar_pt_test2 <- data.frame(pt_type,pt_ages,pt_ratr)
#what is our method of measuring model predictive power
sumsq_p[i] <- sum((predict(outp,MPO_ar_pt_test2)-MPO_ar_pt_test2$pt_ratr)^2)
out<-lm(ratr~ages+I(ages^2)+I(ages^3)+I(ages^4),data=MPO_ar_pt_train)
sumsq[i] <- sum((predict(out,MPO_ar_pt_test)-MPO_ar_pt_test$ratr)^2)
}
mean_SS_t[j+1-13,1] <- mean(sumsq_p)
mean_SS_t[j+1-13,2] <- mean(sumsq)
print(j)
}
sumsq_p[i] <- sum((predict(outp,MPO_ar_pt_test2)-MPO_ar_pt_test2$pt_ratr)^2)
head(clss_kmeans)
?kmeans
#combine data needed from pre-2020 and post-2020
#>15 and 3 groups for 3 tiers of players
#>25 and 5 groups for 5 types of players
#import data
data_nc19 <- read.csv("data_nc19.csv",header=T)
data_nc <- read.csv("data_nc.csv",header=T)
data_nc_comb <- rbind(data_nc19[,c(3,19:26)],data_nc[,c(3,23:30)])
clustd<-data_nc_comb
fhz<-tapply(clustd$Fairway.Hitsz,clustd$Name,mean,na.rm=T)
c1z<-tapply(clustd$Circle.1z,clustd$Name,mean,na.rm=T)
c2z<-tapply(clustd$Circle.2z,clustd$Name,mean,na.rm=T)
sz<-tapply(clustd$Scramblez,clustd$Name,mean,na.rm=T)
c1pz<-tapply(clustd$Circle.1.Puttingz,clustd$Name,mean,na.rm=T)
c2pz<-tapply(clustd$Circle.2.Puttingz,clustd$Name,mean,na.rm=T)
obz<-tapply(clustd$OBz,clustd$Name,mean,na.rm=T)
names<-names(fhz)
#create dataset for clustering
clustdm<-data.frame(fhz,c1z,c2z,sz,c1pz,c2pz,obz,names)
clustdm<-clustdm[table(data_nc_comb$Name)>5,] #removing players who played fewer than 5 events
clustdm<-clustdm[complete.cases(clustdm),]
#number of clusters (classes or player types)
nc <- c(2:6)
#############################
###hierarchical clustering###
#############################
dist_type <- c("euclidean","maximum","manhattan")
nc_dist_type <- expand.grid(nc,dist_type)
clss_hc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_dist_type)))
for (i in 1:nrow(nc_dist_type)) {
dist_mat <- dist(clustdm[,1:7], method = nc_dist_type[i,2])
hclust_avg <- hclust(dist_mat, method = 'average')
#plot(hclust_avg)
cut_avg <- cutree(hclust_avg, k = nc_dist_type[i,1])
clss_hc[,i] <- as.numeric(cut_avg)
names(clss_hc)[i] <- paste(nc_dist_type[i,1],nc_dist_type[i,2],sep="_")
}
#require(dendextend)
#suppressPackageStartupMessages(library(dendextend))
#avg_dend_obj <- as.dendrogram(hclust_avg)
#avg_col_dend <- color_branches(avg_dend_obj, h = 5)
#plot(avg_col_dend)
####################
###GMM clustering###
####################
require(ClusterR)
#finding number of classes
opt_gmm = Optimal_Clusters_GMM(clustdm[,1:7], max_clusters = 10, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
#generate predictions for 2-6 classes
nc <- c(2:6) #setting number of clusters
clss <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=length(nc))) #dataframe for output from models
for (i in 1:length(nc)) { #setup loop to run 5 times
#generating classes
gmm = GMM(clustdm[,1:7], nc[i], dist_mode = "maha_dist", seed_mode = "random_subset", km_iter = 10,
em_iter = 10, verbose = F)
#predicting class
pr = predict_GMM(clustdm[,1:7], gmm$centroids, gmm$covariance_matrices, gmm$weights)
#creating vector of class predictions
clss[,i] <- pr$cluster_labels
names(clss)[i] <- paste("nc",nc[i],sep="")
}
#plotting GMM results
require(ggplot2)
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels))) + geom_point(size=3)
#spectral clustering
#predictions for 2-6 classes and kernels
require(kernlab)
krn <- c("rbfdot","polydot","laplacedot","anovadot","splinedot")
nc_krn <- expand.grid(nc,krn)
clss_sc <- as.data.frame(matrix(0,nrow=nrow(clustdm),ncol=nrow(nc_krn)))
for (i in 1:nrow(nc_krn)) {
sc <- specc(as.matrix(clustdm[,1:7]), centers = nc_krn[i,1], kernel = nc_krn[i,2], kpar = "local")
clss_sc[,i] <- as.numeric(sc@.Data)
names(clss_sc)[i] <- paste(nc_krn[i,1],nc_krn[i,2],sep="_")
}
#plotting SC results
ggplot(clustdm, aes(x=c1z, y = c1pz, color = factor(cluster_labels_SC))) + geom_point(size=3)
require(lme4)
require(lmerTest)
require(MuMIn)
require(cAIC4)
# read in age and PDGA data
MPO_ar <- read.csv("MPO_ar.csv",header=T)
#MPO_ar dataset comes from "MPO_Aging.r" script
MPO_ar$years<-scale(MPO_ar$year)
MPO_ar$ages<-scale(MPO_ar$Age)
#combine ratings and classes (data comes from Player_type.r) for cross-validation of ratings
#can I make this betterc(faster) by pasting all at once then just using match to subset rows?
MPO_ar_tune <- data.frame(matrix(0,nrow=nrow(MPO_ar),ncol=ncol(cbind(clss,clss_sc,clss_hc))))
for (i in 1:nrow(MPO_ar))
{
if (any(grepl(MPO_ar$Name[i],clustdm_cv$names))) {MPO_ar_tune[i,]<-as.numeric(clustdm_cv[grep(paste(MPO_ar$Name_f[i],MPO_ar$Name[i],sep = " "),clustdm_cv$names),c(9:ncol(clustdm_cv))])}
else (MPO_ar_tune[i,]<-"NA")
}
#remove NAs and player type 3 (not enough age data for player type 3)
#MPO_ar_pt <- subset(MPO_ar,MPO_ar$ptype!="NA" & MPO_ar$ptype!="3")
# run cross-validation to compare predictions of age curves including
# player type versus excluding player type
#create dataframe for tuning and remove NAs
MPO_ar_tune2 <- data.frame(MPO_ar,MPO_ar_tune)
names(MPO_ar_tune2)[(ncol(MPO_ar_tune2)-44):ncol(MPO_ar_tune2)] <- names(clustdm_cv)[(ncol(clustdm_cv)-44):ncol(clustdm_cv)]
MPO_ar_pt <- subset(MPO_ar_tune2,MPO_ar_tune2$nc2!="NA")
#run loops
mean_SS_t <- data.frame(matrix(0,nrow=ncol(cbind(clss,clss_sc,clss_hc,clss_kmeans)),ncol=2))
for (j in (ncol(MPO_ar_tune2)-44):ncol(MPO_ar_tune2)) {
sumsq_p<-numeric(1000)
sumsq<-numeric(1000)
#training size
tz <- round(length(unique(MPO_ar_pt$PDGA))*.8)
for (i in 1:1000){
train <- sample(unique(MPO_ar_pt$PDGA),tz)
test <- unique(MPO_ar_pt$PDGA)[!unique(MPO_ar_pt$PDGA) %in% train]
MPO_ar_pt_train <- MPO_ar_pt[MPO_ar_pt$PDGA %in% train,]
MPO_ar_pt_test <- MPO_ar_pt[MPO_ar_pt$PDGA %in% test,]
pt_type <- MPO_ar_pt_train[,j]
pt_ages <- MPO_ar_pt_train$ages
pt_ratr <- MPO_ar_pt_train$ratr
MPO_ar_pt_train2 <- data.frame(pt_type,pt_ages,pt_ratr)
outp<-lm(pt_ratr~pt_type*pt_ages+pt_type*I(pt_ages^2)+pt_type*I(pt_ages^3)+pt_type*I(pt_ages^4),data=MPO_ar_pt_train2)
pt_type <- MPO_ar_pt_test[,j]
pt_ages <- MPO_ar_pt_test$ages
pt_ratr <- MPO_ar_pt_test$ratr
MPO_ar_pt_test2 <- data.frame(pt_type,pt_ages,pt_ratr)
#what is our method of measuring model predictive power
sumsq_p[i] <- sum((predict(outp,MPO_ar_pt_test2)-MPO_ar_pt_test2$pt_ratr)^2)
out<-lm(ratr~ages+I(ages^2)+I(ages^3)+I(ages^4),data=MPO_ar_pt_train)
sumsq[i] <- sum((predict(out,MPO_ar_pt_test)-MPO_ar_pt_test$ratr)^2)
for (i in 1:1000){
train <- sample(unique(MPO_ar_pt$PDGA),tz)
test <- unique(MPO_ar_pt$PDGA)[!unique(MPO_ar_pt$PDGA) %in% train]
MPO_ar_pt_train <- MPO_ar_pt[MPO_ar_pt$PDGA %in% train,]
MPO_ar_pt_test <- MPO_ar_pt[MPO_ar_pt$PDGA %in% test,]
pt_type <- MPO_ar_pt_train[,j]
pt_ages <- MPO_ar_pt_train$ages
pt_ratr <- MPO_ar_pt_train$ratr
MPO_ar_pt_train2 <- data.frame(pt_type,pt_ages,pt_ratr)
outp<-lm(pt_ratr~pt_type*pt_ages+pt_type*I(pt_ages^2)+pt_type*I(pt_ages^3)+pt_type*I(pt_ages^4),data=MPO_ar_pt_train2)
pt_type <- MPO_ar_pt_test[,j]
pt_ages <- MPO_ar_pt_test$ages
pt_ratr <- MPO_ar_pt_test$ratr
MPO_ar_pt_test2 <- data.frame(pt_type,pt_ages,pt_ratr)
#what is our method of measuring model predictive power
sumsq_p[i] <- sum((predict(outp,MPO_ar_pt_test2)-MPO_ar_pt_test2$pt_ratr)^2)
out<-lm(ratr~ages+I(ages^2)+I(ages^3)+I(ages^4),data=MPO_ar_pt_train)
sumsq[i] <- sum((predict(out,MPO_ar_pt_test)-MPO_ar_pt_test$ratr)^2)
}
mean_SS_t[j+1-13,1] <- mean(sumsq_p)
mean_SS_t[j+1-13,2] <- mean(sumsq)
head(sumsq_p)
